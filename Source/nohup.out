  File "Douban.py", line 42
    C.layers.BatchNormalization(),
    ^
SyntaxError: invalid syntax
Selected GPU[0] GeForce GTX 980 Ti as the process wide default device.
Vocabulary size : 129746
Number of labels: 2
Training size: 274064
Traceback (most recent call last):
  File "Douban.py", line 224, in <module>
    main()
  File "Douban.py", line 217, in main
    model = create_model(y)(x)
  File "Douban.py", line 41, in create_model
    C.layers.Recurrence(C.layers.GRU(parameters["hiddemDim"] // 2)),
KeyError: 'hiddemDim'
Selected GPU[0] GeForce GTX 980 Ti as the process wide default device.
-------------------------------------------------------------------
Build info: 

		Built time: Sep 15 2017 07:31:01
		Last modified date: Fri Sep 15 04:28:48 2017
		Build type: release
		Build target: GPU
		With 1bit-SGD: yes
		With ASGD: yes
		Math lib: mkl
		CUDA version: 9.0.0
		CUDNN version: 6.0.21
		Build Branch: HEAD
		Build SHA1: 23878e5d1f73180d6564b6f907b14fe5f53513bb
		MPI distribution: Open MPI
		MPI version: 1.10.7
-------------------------------------------------------------------
Vocabulary size : 129746
Number of labels: 2
Training size: 274064
(129746, 600)
[ 0.  0.]
Training 78187352 parameters in 11 parameter tensors.
Learning rate per minibatch: 0.1536
 Minibatch[   1- 100]: loss = 5.573818 * 2541, metric = 44.39% * 2541;
 Minibatch[ 101- 200]: loss = 2.657327 * 2719, metric = 38.40% * 2719;
 Minibatch[ 201- 300]: loss = 1.970146 * 2523, metric = 36.82% * 2523;
 Minibatch[ 301- 400]: loss = 1.322163 * 2710, metric = 36.13% * 2710;
 Minibatch[ 401- 500]: loss = 0.977609 * 2571, metric = 34.81% * 2571;
 Minibatch[ 501- 600]: loss = 0.783611 * 2412, metric = 34.54% * 2412;
 Minibatch[ 601- 700]: loss = 0.664432 * 2620, metric = 32.56% * 2620;
 Minibatch[ 701- 800]: loss = 0.605797 * 2538, metric = 31.09% * 2538;
 Minibatch[ 801- 900]: loss = 0.599639 * 2579, metric = 31.17% * 2579;
 Minibatch[ 901-1000]: loss = 0.572073 * 2558, metric = 29.01% * 2558;
 Minibatch[1001-1100]: loss = 0.570529 * 2582, metric = 28.78% * 2582;
 Minibatch[1101-1200]: loss = 0.575601 * 2646, metric = 29.67% * 2646;
 Minibatch[1201-1300]: loss = 0.567990 * 2498, metric = 28.06% * 2498;
 Minibatch[1301-1400]: loss = 0.547098 * 2558, metric = 26.47% * 2558;
 Minibatch[1401-1500]: loss = 0.560034 * 2611, metric = 27.38% * 2611;
 Minibatch[1501-1600]: loss = 0.549993 * 2553, metric = 28.40% * 2553;
 Minibatch[1601-1700]: loss = 0.516277 * 2577, metric = 25.18% * 2577;
 Minibatch[1701-1800]: loss = 0.535381 * 2663, metric = 26.44% * 2663;
 Minibatch[1801-1900]: loss = 0.538117 * 2591, metric = 26.79% * 2591;
 Minibatch[1901-2000]: loss = 0.531567 * 2602, metric = 25.79% * 2602;
 Minibatch[2001-2100]: loss = 0.535637 * 2591, metric = 26.55% * 2591;
 Minibatch[2101-2200]: loss = 0.506358 * 2498, metric = 23.86% * 2498;
 Minibatch[2201-2300]: loss = 0.515214 * 2726, metric = 24.54% * 2726;
 Minibatch[2301-2400]: loss = 0.499749 * 2658, metric = 24.98% * 2658;
 Minibatch[2401-2500]: loss = 0.515553 * 2542, metric = 25.49% * 2542;
 Minibatch[2501-2600]: loss = 0.505949 * 2796, metric = 25.14% * 2796;
 Minibatch[2601-2700]: loss = 0.486726 * 2642, metric = 23.88% * 2642;
 Minibatch[2701-2800]: loss = 0.511748 * 2416, metric = 24.42% * 2416;
 Minibatch[2801-2900]: loss = 0.486894 * 2680, metric = 23.62% * 2680;
 Minibatch[2901-3000]: loss = 0.507869 * 2641, metric = 24.01% * 2641;
 Minibatch[3001-3100]: loss = 0.506501 * 2437, metric = 24.29% * 2437;
 Minibatch[3101-3200]: loss = 0.474082 * 2709, metric = 22.26% * 2709;
 Minibatch[3201-3300]: loss = 0.479872 * 2658, metric = 22.42% * 2658;
 Minibatch[3301-3400]: loss = 0.472920 * 2578, metric = 21.41% * 2578;
Finished Epoch[1 of 20]: [Training] loss = 0.821740 * 90002, metric = 28.04% * 90002 90.563s (993.8 samples/s);
 Evaluation Minibatch[   1- 500]: metric = 19.79% * 13062;
 Evaluation Minibatch[ 501-1000]: metric = 20.04% * 12994;
 Evaluation Minibatch[1001-1500]: metric = 20.16% * 12958;
 Evaluation Minibatch[1501-2000]: metric = 20.17% * 13178;
 Evaluation Minibatch[2001-2500]: metric = 19.93% * 12833;
Finished Evaluation [1]: Minibatch[1-2637]: metric = 20.00% * 68517;
 Minibatch[   1- 100]: loss = 0.485854 * 2632, metric = 23.06% * 2632;
 Minibatch[ 101- 200]: loss = 0.485926 * 2541, metric = 22.12% * 2541;
 Minibatch[ 201- 300]: loss = 0.455762 * 2628, metric = 21.27% * 2628;
 Minibatch[ 301- 400]: loss = 0.453522 * 2695, metric = 21.60% * 2695;
 Minibatch[ 401- 500]: loss = 0.450422 * 2551, metric = 21.64% * 2551;
 Minibatch[ 501- 600]: loss = 0.469112 * 2500, metric = 21.84% * 2500;
 Minibatch[ 601- 700]: loss = 0.462488 * 2648, metric = 21.83% * 2648;
 Minibatch[ 701- 800]: loss = 0.477781 * 2527, metric = 23.62% * 2527;
 Minibatch[ 801- 900]: loss = 0.432621 * 2574, metric = 19.54% * 2574;
 Minibatch[ 901-1000]: loss = 0.455743 * 2579, metric = 21.09% * 2579;
 Minibatch[1001-1100]: loss = 0.480163 * 2615, metric = 23.10% * 2615;
 Minibatch[1101-1200]: loss = 0.473404 * 2497, metric = 23.11% * 2497;
 Minibatch[1201-1300]: loss = 0.448425 * 2486, metric = 20.39% * 2486;
 Minibatch[1301-1400]: loss = 0.465177 * 2537, metric = 21.72% * 2537;
 Minibatch[1401-1500]: loss = 0.428613 * 2642, metric = 19.15% * 2642;
 Minibatch[1501-1600]: loss = 0.443498 * 2582, metric = 20.10% * 2582;
 Minibatch[1601-1700]: loss = 0.437485 * 2549, metric = 19.81% * 2549;
 Minibatch[1701-1800]: loss = 0.457315 * 2466, metric = 21.41% * 2466;
 Minibatch[1801-1900]: loss = 0.444589 * 2495, metric = 20.80% * 2495;
 Minibatch[1901-2000]: loss = 0.438995 * 2583, metric = 20.56% * 2583;
 Minibatch[2001-2100]: loss = 0.440377 * 2601, metric = 20.84% * 2601;
 Minibatch[2101-2200]: loss = 0.427559 * 2598, metric = 18.94% * 2598;
 Minibatch[2201-2300]: loss = 0.431731 * 2635, metric = 19.51% * 2635;
 Minibatch[2301-2400]: loss = 0.442702 * 2629, metric = 20.01% * 2629;
 Minibatch[2401-2500]: loss = 0.427711 * 2544, metric = 19.50% * 2544;
 Minibatch[2501-2600]: loss = 0.424843 * 2751, metric = 19.74% * 2751;
 Minibatch[2601-2700]: loss = 0.444990 * 2489, metric = 21.17% * 2489;
 Minibatch[2701-2800]: loss = 0.418126 * 2618, metric = 19.52% * 2618;
 Minibatch[2801-2900]: loss = 0.421485 * 2647, metric = 20.55% * 2647;
 Minibatch[2901-3000]: loss = 0.426043 * 2521, metric = 19.08% * 2521;
 Minibatch[3001-3100]: loss = 0.444781 * 2417, metric = 19.69% * 2417;
 Minibatch[3101-3200]: loss = 0.466323 * 2466, metric = 21.49% * 2466;
 Minibatch[3201-3300]: loss = 0.404729 * 2607, metric = 18.87% * 2607;
 Minibatch[3301-3400]: loss = 0.446612 * 2729, metric = 20.59% * 2729;
 Minibatch[3401-3500]: loss = 0.419451 * 2432, metric = 18.83% * 2432;
Finished Epoch[2 of 20]: [Training] loss = 0.446590 * 90011, metric = 20.74% * 90011 124.616s (722.3 samples/s);
 Evaluation Minibatch[   1- 500]: metric = 16.28% * 13062;
 Evaluation Minibatch[ 501-1000]: metric = 16.14% * 12994;
 Evaluation Minibatch[1001-1500]: metric = 16.66% * 12958;
 Evaluation Minibatch[1501-2000]: metric = 16.46% * 13178;
 Evaluation Minibatch[2001-2500]: metric = 16.52% * 12833;
Finished Evaluation [2]: Minibatch[1-2637]: metric = 16.39% * 68517;
 Minibatch[   1- 100]: loss = 0.407413 * 2706, metric = 18.33% * 2706;
 Minibatch[ 101- 200]: loss = 0.426944 * 2495, metric = 19.32% * 2495;
 Minibatch[ 201- 300]: loss = 0.398957 * 2514, metric = 17.74% * 2514;
 Minibatch[ 301- 400]: loss = 0.400418 * 2601, metric = 18.72% * 2601;
 Minibatch[ 401- 500]: loss = 0.405887 * 2672, metric = 19.24% * 2672;
 Minibatch[ 501- 600]: loss = 0.415185 * 2467, metric = 18.36% * 2467;
 Minibatch[ 601- 700]: loss = 0.420927 * 2707, metric = 18.25% * 2707;
 Minibatch[ 701- 800]: loss = 0.405089 * 2588, metric = 17.77% * 2588;
 Minibatch[ 801- 900]: loss = 0.416729 * 2494, metric = 19.13% * 2494;
 Minibatch[ 901-1000]: loss = 0.422105 * 2532, metric = 19.47% * 2532;
 Minibatch[1001-1100]: loss = 0.432639 * 2442, metric = 20.39% * 2442;
 Minibatch[1101-1200]: loss = 0.397325 * 2630, metric = 17.83% * 2630;
 Minibatch[1201-1300]: loss = 0.392419 * 2603, metric = 17.71% * 2603;
 Minibatch[1301-1400]: loss = 0.409727 * 2647, metric = 18.13% * 2647;
 Minibatch[1401-1500]: loss = 0.415946 * 2551, metric = 19.17% * 2551;
 Minibatch[1501-1600]: loss = 0.400656 * 2598, metric = 18.55% * 2598;
 Minibatch[1601-1700]: loss = 0.421921 * 2602, metric = 20.02% * 2602;
 Minibatch[1701-1800]: loss = 0.412130 * 2536, metric = 18.89% * 2536;
 Minibatch[1801-1900]: loss = 0.395031 * 2580, metric = 18.18% * 2580;
 Minibatch[1901-2000]: loss = 0.390319 * 2748, metric = 17.87% * 2748;
 Minibatch[2001-2100]: loss = 0.422784 * 2616, metric = 19.72% * 2616;
 Minibatch[2101-2200]: loss = 0.399299 * 2786, metric = 17.41% * 2786;
 Minibatch[2201-2300]: loss = 0.393335 * 2630, metric = 18.40% * 2630;
 Minibatch[2301-2400]: loss = 0.381582 * 2593, metric = 16.62% * 2593;
 Minibatch[2401-2500]: loss = 0.405791 * 2717, metric = 18.51% * 2717;
 Minibatch[2501-2600]: loss = 0.410470 * 2653, metric = 18.85% * 2653;
 Minibatch[2601-2700]: loss = 0.391776 * 2722, metric = 17.49% * 2722;
 Minibatch[2701-2800]: loss = 0.415669 * 2516, metric = 19.59% * 2516;
 Minibatch[2801-2900]: loss = 0.384712 * 2498, metric = 16.85% * 2498;
 Minibatch[2901-3000]: loss = 0.382959 * 2670, metric = 17.64% * 2670;
 Minibatch[3001-3100]: loss = 0.375993 * 2651, metric = 16.97% * 2651;
 Minibatch[3101-3200]: loss = 0.392019 * 2698, metric = 18.01% * 2698;
 Minibatch[3201-3300]: loss = 0.407114 * 2560, metric = 18.55% * 2560;
 Minibatch[3301-3400]: loss = 0.392486 * 2399, metric = 17.67% * 2399;
Finished Epoch[3 of 20]: [Training] loss = 0.403872 * 89991, metric = 18.36% * 89991 114.790s (784.0 samples/s);
 Evaluation Minibatch[   1- 500]: metric = 15.03% * 13062;
 Evaluation Minibatch[ 501-1000]: metric = 14.57% * 12994;
 Evaluation Minibatch[1001-1500]: metric = 15.48% * 12958;
 Evaluation Minibatch[1501-2000]: metric = 14.75% * 13178;
 Evaluation Minibatch[2001-2500]: metric = 15.30% * 12833;
Finished Evaluation [3]: Minibatch[1-2637]: metric = 15.06% * 68517;
 Minibatch[   1- 100]: loss = 0.384368 * 2706, metric = 17.37% * 2706;
 Minibatch[ 101- 200]: loss = 0.371899 * 2546, metric = 17.01% * 2546;
 Minibatch[ 201- 300]: loss = 0.348618 * 2678, metric = 15.16% * 2678;
 Minibatch[ 301- 400]: loss = 0.385853 * 2590, metric = 16.76% * 2590;
 Minibatch[ 401- 500]: loss = 0.364403 * 2520, metric = 15.91% * 2520;
 Minibatch[ 501- 600]: loss = 0.353248 * 2634, metric = 15.60% * 2634;
 Minibatch[ 601- 700]: loss = 0.367163 * 2504, metric = 16.77% * 2504;
 Minibatch[ 701- 800]: loss = 0.386087 * 2452, metric = 17.17% * 2452;
 Minibatch[ 801- 900]: loss = 0.356169 * 2645, metric = 16.33% * 2645;
 Minibatch[ 901-1000]: loss = 0.359026 * 2460, metric = 15.12% * 2460;
 Minibatch[1001-1100]: loss = 0.368168 * 2701, metric = 16.59% * 2701;
 Minibatch[1101-1200]: loss = 0.357338 * 2756, metric = 15.46% * 2756;
 Minibatch[1201-1300]: loss = 0.371097 * 2577, metric = 16.80% * 2577;
 Minibatch[1301-1400]: loss = 0.347136 * 2643, metric = 14.95% * 2643;
 Minibatch[1401-1500]: loss = 0.358569 * 2654, metric = 15.83% * 2654;
 Minibatch[1501-1600]: loss = 0.362163 * 2590, metric = 16.14% * 2590;
 Minibatch[1601-1700]: loss = 0.390395 * 2428, metric = 17.13% * 2428;
 Minibatch[1701-1800]: loss = 0.358110 * 2691, metric = 15.72% * 2691;
 Minibatch[1801-1900]: loss = 0.356345 * 2736, metric = 16.48% * 2736;
 Minibatch[1901-2000]: loss = 0.371994 * 2743, metric = 17.10% * 2743;
 Minibatch[2001-2100]: loss = 0.353020 * 2658, metric = 15.84% * 2658;
 Minibatch[2101-2200]: loss = 0.369984 * 2580, metric = 16.09% * 2580;
 Minibatch[2201-2300]: loss = 0.374906 * 2606, metric = 16.73% * 2606;
 Minibatch[2301-2400]: loss = 0.360305 * 2609, metric = 16.06% * 2609;
 Minibatch[2401-2500]: loss = 0.390168 * 2549, metric = 17.58% * 2549;
 Minibatch[2501-2600]: loss = 0.356851 * 2657, metric = 15.84% * 2657;
 Minibatch[2601-2700]: loss = 0.388019 * 2496, metric = 17.43% * 2496;
 Minibatch[2701-2800]: loss = 0.356644 * 2730, metric = 15.82% * 2730;
 Minibatch[2801-2900]: loss = 0.365591 * 2582, metric = 15.84% * 2582;
 Minibatch[2901-3000]: loss = 0.361828 * 2470, metric = 15.71% * 2470;
 Minibatch[3001-3100]: loss = 0.378118 * 2531, metric = 16.12% * 2531;
 Minibatch[3101-3200]: loss = 0.348556 * 2460, metric = 14.55% * 2460;
 Minibatch[3201-3300]: loss = 0.377638 * 2503, metric = 16.94% * 2503;
 Minibatch[3301-3400]: loss = 0.365977 * 2597, metric = 16.48% * 2597;
Finished Epoch[4 of 20]: [Training] loss = 0.366498 * 90011, metric = 16.25% * 90011 114.139s (788.6 samples/s);
 Evaluation Minibatch[   1- 500]: metric = 14.01% * 13062;
 Evaluation Minibatch[ 501-1000]: metric = 13.11% * 12994;
 Evaluation Minibatch[1001-1500]: metric = 14.41% * 12958;
 Evaluation Minibatch[1501-2000]: metric = 13.55% * 13178;
 Evaluation Minibatch[2001-2500]: metric = 13.38% * 12833;
Finished Evaluation [4]: Minibatch[1-2637]: metric = 13.72% * 68517;
Learning rate per minibatch: 0.0768
 Minibatch[   1- 100]: loss = 0.343403 * 2616, metric = 14.49% * 2616;
 Minibatch[ 101- 200]: loss = 0.354832 * 2423, metric = 15.77% * 2423;
 Minibatch[ 201- 300]: loss = 0.349005 * 2597, metric = 15.71% * 2597;
 Minibatch[ 301- 400]: loss = 0.343039 * 2634, metric = 14.77% * 2634;
 Minibatch[ 401- 500]: loss = 0.346784 * 2714, metric = 14.92% * 2714;
 Minibatch[ 501- 600]: loss = 0.321686 * 2569, metric = 13.94% * 2569;
 Minibatch[ 601- 700]: loss = 0.322585 * 2640, metric = 13.98% * 2640;
 Minibatch[ 701- 800]: loss = 0.352412 * 2514, metric = 15.67% * 2514;
 Minibatch[ 801- 900]: loss = 0.338975 * 2466, metric = 15.17% * 2466;
 Minibatch[ 901-1000]: loss = 0.347315 * 2365, metric = 14.46% * 2365;
 Minibatch[1001-1100]: loss = 0.344365 * 2646, metric = 14.93% * 2646;
 Minibatch[1101-1200]: loss = 0.328580 * 2574, metric = 13.79% * 2574;
 Minibatch[1201-1300]: loss = 0.338583 * 2546, metric = 14.85% * 2546;
 Minibatch[1301-1400]: loss = 0.333962 * 2598, metric = 14.59% * 2598;
 Minibatch[1401-1500]: loss = 0.340373 * 2616, metric = 13.91% * 2616;
 Minibatch[1501-1600]: loss = 0.338129 * 2564, metric = 14.47% * 2564;
 Minibatch[1601-1700]: loss = 0.326393 * 2755, metric = 13.61% * 2755;
 Minibatch[1701-1800]: loss = 0.338795 * 2596, metric = 14.56% * 2596;
 Minibatch[1801-1900]: loss = 0.335544 * 2468, metric = 13.29% * 2468;
 Minibatch[1901-2000]: loss = 0.326796 * 2593, metric = 13.77% * 2593;
 Minibatch[2001-2100]: loss = 0.353808 * 2621, metric = 14.65% * 2621;
 Minibatch[2101-2200]: loss = 0.331788 * 2581, metric = 14.65% * 2581;
 Minibatch[2201-2300]: loss = 0.336218 * 2644, metric = 15.13% * 2644;
 Minibatch[2301-2400]: loss = 0.313036 * 2729, metric = 14.36% * 2729;
 Minibatch[2401-2500]: loss = 0.332466 * 2559, metric = 13.52% * 2559;
 Minibatch[2501-2600]: loss = 0.332602 * 2606, metric = 14.47% * 2606;
 Minibatch[2601-2700]: loss = 0.327204 * 2419, metric = 14.26% * 2419;
 Minibatch[2701-2800]: loss = 0.341336 * 2556, metric = 15.49% * 2556;
 Minibatch[2801-2900]: loss = 0.322623 * 2629, metric = 13.73% * 2629;
 Minibatch[2901-3000]: loss = 0.320011 * 2587, metric = 13.84% * 2587;
 Minibatch[3001-3100]: loss = 0.328001 * 2575, metric = 14.14% * 2575;
 Minibatch[3101-3200]: loss = 0.325907 * 2576, metric = 13.98% * 2576;
 Minibatch[3201-3300]: loss = 0.334857 * 2699, metric = 14.12% * 2699;
 Minibatch[3301-3400]: loss = 0.338511 * 2551, metric = 14.19% * 2551;
Finished Epoch[5 of 20]: [Training] loss = 0.335389 * 89993, metric = 14.43% * 89993 127.992s (703.1 samples/s);
 Evaluation Minibatch[   1- 500]: metric = 12.74% * 13062;
 Evaluation Minibatch[ 501-1000]: metric = 12.34% * 12994;
 Evaluation Minibatch[1001-1500]: metric = 13.24% * 12958;
 Evaluation Minibatch[1501-2000]: metric = 12.44% * 13178;
 Evaluation Minibatch[2001-2500]: metric = 12.34% * 12833;
Finished Evaluation [5]: Minibatch[1-2637]: metric = 12.66% * 68517;
 Minibatch[   1- 100]: loss = 0.330480 * 2569, metric = 13.62% * 2569;
 Minibatch[ 101- 200]: loss = 0.324492 * 2600, metric = 13.31% * 2600;
 Minibatch[ 201- 300]: loss = 0.327159 * 2610, metric = 14.06% * 2610;
 Minibatch[ 301- 400]: loss = 0.332986 * 2497, metric = 14.10% * 2497;
 Minibatch[ 401- 500]: loss = 0.338516 * 2527, metric = 14.29% * 2527;
 Minibatch[ 501- 600]: loss = 0.320793 * 2514, metric = 14.28% * 2514;
 Minibatch[ 601- 700]: loss = 0.298817 * 2596, metric = 13.10% * 2596;
 Minibatch[ 701- 800]: loss = 0.323827 * 2556, metric = 13.85% * 2556;
 Minibatch[ 801- 900]: loss = 0.322224 * 2650, metric = 13.47% * 2650;
 Minibatch[ 901-1000]: loss = 0.334840 * 2472, metric = 14.72% * 2472;
 Minibatch[1001-1100]: loss = 0.318571 * 2404, metric = 13.69% * 2404;
 Minibatch[1101-1200]: loss = 0.334771 * 2649, metric = 14.50% * 2649;
 Minibatch[1201-1300]: loss = 0.306403 * 2497, metric = 12.82% * 2497;
 Minibatch[1301-1400]: loss = 0.322413 * 2551, metric = 13.92% * 2551;
 Minibatch[1401-1500]: loss = 0.317300 * 2555, metric = 13.66% * 2555;
 Minibatch[1501-1600]: loss = 0.313023 * 2618, metric = 13.87% * 2618;
 Minibatch[1601-1700]: loss = 0.325385 * 2642, metric = 14.42% * 2642;
 Minibatch[1701-1800]: loss = 0.314294 * 2668, metric = 13.83% * 2668;
 Minibatch[1801-1900]: loss = 0.302072 * 2562, metric = 12.53% * 2562;
 Minibatch[1901-2000]: loss = 0.317694 * 2592, metric = 14.16% * 2592;
 Minibatch[2001-2100]: loss = 0.323413 * 2669, metric = 14.05% * 2669;
 Minibatch[2101-2200]: loss = 0.331856 * 2479, metric = 14.08% * 2479;
 Minibatch[2201-2300]: loss = 0.321430 * 2541, metric = 13.54% * 2541;
 Minibatch[2301-2400]: loss = 0.311109 * 2575, metric = 13.59% * 2575;
 Minibatch[2401-2500]: loss = 0.321973 * 2747, metric = 13.51% * 2747;
 Minibatch[2501-2600]: loss = 0.305032 * 2597, metric = 13.36% * 2597;
 Minibatch[2601-2700]: loss = 0.328479 * 2649, metric = 14.35% * 2649;
 Minibatch[2701-2800]: loss = 0.308044 * 2584, metric = 13.62% * 2584;
 Minibatch[2801-2900]: loss = 0.321056 * 2641, metric = 14.54% * 2641;
 Minibatch[2901-3000]: loss = 0.303369 * 2659, metric = 13.13% * 2659;
 Minibatch[3001-3100]: loss = 0.309219 * 2752, metric = 13.34% * 2752;
 Minibatch[3101-3200]: loss = 0.312997 * 2563, metric = 13.11% * 2563;
 Minibatch[3201-3300]: loss = 0.316837 * 2653, metric = 12.89% * 2653;
 Minibatch[3301-3400]: loss = 0.313271 * 2747, metric = 13.69% * 2747;
Finished Epoch[6 of 20]: [Training] loss = 0.319518 * 89993, metric = 13.74% * 89993 111.530s (806.9 samples/s);
 Evaluation Minibatch[   1- 500]: metric = 12.48% * 13062;
 Evaluation Minibatch[ 501-1000]: metric = 11.97% * 12994;
 Evaluation Minibatch[1001-1500]: metric = 12.69% * 12958;
 Evaluation Minibatch[1501-2000]: metric = 11.96% * 13178;
 Evaluation Minibatch[2001-2500]: metric = 12.04% * 12833;
Finished Evaluation [6]: Minibatch[1-2637]: metric = 12.25% * 68517;
 Minibatch[   1- 100]: loss = 0.319625 * 2721, metric = 14.11% * 2721;
 Minibatch[ 101- 200]: loss = 0.291600 * 2654, metric = 12.21% * 2654;
 Minibatch[ 201- 300]: loss = 0.310956 * 2526, metric = 12.98% * 2526;
 Minibatch[ 301- 400]: loss = 0.305137 * 2559, metric = 12.86% * 2559;
 Minibatch[ 401- 500]: loss = 0.286458 * 2569, metric = 11.95% * 2569;
 Minibatch[ 501- 600]: loss = 0.275680 * 2496, metric = 11.22% * 2496;
 Minibatch[ 601- 700]: loss = 0.300061 * 2568, metric = 12.77% * 2568;
 Minibatch[ 701- 800]: loss = 0.296141 * 2568, metric = 12.23% * 2568;
 Minibatch[ 801- 900]: loss = 0.274726 * 2760, metric = 11.52% * 2760;
 Minibatch[ 901-1000]: loss = 0.294932 * 2615, metric = 12.81% * 2615;
 Minibatch[1001-1100]: loss = 0.309477 * 2384, metric = 13.17% * 2384;
 Minibatch[1101-1200]: loss = 0.302208 * 2609, metric = 13.61% * 2609;
 Minibatch[1201-1300]: loss = 0.286806 * 2608, metric = 12.27% * 2608;
 Minibatch[1301-1400]: loss = 0.287716 * 2551, metric = 11.84% * 2551;
 Minibatch[1401-1500]: loss = 0.311415 * 2583, metric = 13.12% * 2583;
 Minibatch[1501-1600]: loss = 0.295426 * 2673, metric = 12.72% * 2673;
 Minibatch[1601-1700]: loss = 0.300727 * 2562, metric = 12.57% * 2562;
 Minibatch[1701-1800]: loss = 0.282033 * 2484, metric = 12.36% * 2484;
 Minibatch[1801-1900]: loss = 0.293828 * 2505, metric = 12.02% * 2505;
 Minibatch[1901-2000]: loss = 0.306159 * 2586, metric = 13.42% * 2586;
 Minibatch[2001-2100]: loss = 0.302449 * 2724, metric = 12.89% * 2724;
 Minibatch[2101-2200]: loss = 0.303270 * 2571, metric = 13.30% * 2571;
 Minibatch[2201-2300]: loss = 0.296360 * 2720, metric = 12.21% * 2720;
 Minibatch[2301-2400]: loss = 0.295034 * 2569, metric = 12.88% * 2569;
 Minibatch[2401-2500]: loss = 0.277385 * 2646, metric = 12.17% * 2646;
 Minibatch[2501-2600]: loss = 0.308679 * 2669, metric = 13.15% * 2669;
 Minibatch[2601-2700]: loss = 0.287031 * 2741, metric = 11.89% * 2741;
 Minibatch[2701-2800]: loss = 0.306979 * 2597, metric = 13.25% * 2597;
 Minibatch[2801-2900]: loss = 0.273168 * 2519, metric = 11.79% * 2519;
 Minibatch[2901-3000]: loss = 0.273900 * 2687, metric = 10.83% * 2687;
 Minibatch[3001-3100]: loss = 0.277077 * 2508, metric = 11.44% * 2508;
 Minibatch[3101-3200]: loss = 0.278856 * 2365, metric = 11.21% * 2365;
 Minibatch[3201-3300]: loss = 0.269263 * 2550, metric = 11.14% * 2550;
 Minibatch[3301-3400]: loss = 0.295360 * 2485, metric = 12.72% * 2485;
Finished Epoch[7 of 20]: [Training] loss = 0.292957 * 90005, metric = 12.40% * 90005 113.757s (791.2 samples/s);
 Evaluation Minibatch[   1- 500]: metric = 11.38% * 13062;
 Evaluation Minibatch[ 501-1000]: metric = 11.40% * 12994;
 Evaluation Minibatch[1001-1500]: metric = 12.02% * 12958;
 Evaluation Minibatch[1501-2000]: metric = 11.35% * 13178;
 Evaluation Minibatch[2001-2500]: metric = 12.16% * 12833;
Finished Evaluation [7]: Minibatch[1-2637]: metric = 11.66% * 68517;
 Minibatch[   1- 100]: loss = 0.275569 * 2489, metric = 11.57% * 2489;
 Minibatch[ 101- 200]: loss = 0.293705 * 2455, metric = 12.26% * 2455;
 Minibatch[ 201- 300]: loss = 0.271235 * 2715, metric = 11.38% * 2715;
 Minibatch[ 301- 400]: loss = 0.281613 * 2664, metric = 10.96% * 2664;
 Minibatch[ 401- 500]: loss = 0.271623 * 2576, metric = 11.61% * 2576;
 Minibatch[ 501- 600]: loss = 0.315706 * 2567, metric = 13.48% * 2567;
 Minibatch[ 601- 700]: loss = 0.316681 * 2545, metric = 13.91% * 2545;
 Minibatch[ 701- 800]: loss = 0.287140 * 2559, metric = 12.31% * 2559;
 Minibatch[ 801- 900]: loss = 0.274333 * 2797, metric = 11.23% * 2797;
 Minibatch[ 901-1000]: loss = 0.283142 * 2587, metric = 11.83% * 2587;
 Minibatch[1001-1100]: loss = 0.291472 * 2640, metric = 12.77% * 2640;
 Minibatch[1101-1200]: loss = 0.279932 * 2590, metric = 11.70% * 2590;
 Minibatch[1201-1300]: loss = 0.269416 * 2531, metric = 11.34% * 2531;
 Minibatch[1301-1400]: loss = 0.281892 * 2663, metric = 10.78% * 2663;
 Minibatch[1401-1500]: loss = 0.300232 * 2511, metric = 12.50% * 2511;
 Minibatch[1501-1600]: loss = 0.292046 * 2634, metric = 12.38% * 2634;
 Minibatch[1601-1700]: loss = 0.280692 * 2503, metric = 11.11% * 2503;
 Minibatch[1701-1800]: loss = 0.295102 * 2466, metric = 12.29% * 2466;
 Minibatch[1801-1900]: loss = 0.269259 * 2767, metric = 11.28% * 2767;
 Minibatch[1901-2000]: loss = 0.293232 * 2691, metric = 11.93% * 2691;
 Minibatch[2001-2100]: loss = 0.285508 * 2582, metric = 11.58% * 2582;
 Minibatch[2101-2200]: loss = 0.298904 * 2628, metric = 12.60% * 2628;
 Minibatch[2201-2300]: loss = 0.280268 * 2805, metric = 11.73% * 2805;
 Minibatch[2301-2400]: loss = 0.284548 * 2609, metric = 12.27% * 2609;
 Minibatch[2401-2500]: loss = 0.298525 * 2618, metric = 12.18% * 2618;
 Minibatch[2501-2600]: loss = 0.276475 * 2690, metric = 11.45% * 2690;
 Minibatch[2601-2700]: loss = 0.284645 * 2633, metric = 11.55% * 2633;
 Minibatch[2701-2800]: loss = 0.266558 * 2641, metric = 10.87% * 2641;
 Minibatch[2801-2900]: loss = 0.297603 * 2544, metric = 12.54% * 2544;
 Minibatch[2901-3000]: loss = 0.268653 * 2779, metric = 11.44% * 2779;
 Minibatch[3001-3100]: loss = 0.287933 * 2629, metric = 12.67% * 2629;
 Minibatch[3101-3200]: loss = 0.283581 * 2455, metric = 12.02% * 2455;
 Minibatch[3201-3300]: loss = 0.289259 * 2574, metric = 11.62% * 2574;
 Minibatch[3301-3400]: loss = 0.281956 * 2595, metric = 11.75% * 2595;
Finished Epoch[8 of 20]: [Training] loss = 0.286293 * 90011, metric = 11.93% * 90011 111.933s (804.2 samples/s);
 Evaluation Minibatch[   1- 500]: metric = 11.94% * 13062;
 Evaluation Minibatch[ 501-1000]: metric = 11.28% * 12994;
 Evaluation Minibatch[1001-1500]: metric = 12.40% * 12958;
 Evaluation Minibatch[1501-2000]: metric = 11.44% * 13178;
 Evaluation Minibatch[2001-2500]: metric = 12.11% * 12833;
Finished Evaluation [8]: Minibatch[1-2637]: metric = 11.85% * 68517;
 Minibatch[   1- 100]: loss = 0.323056 * 2545, metric = 13.60% * 2545;
 Minibatch[ 101- 200]: loss = 0.290359 * 2680, metric = 12.65% * 2680;
 Minibatch[ 201- 300]: loss = 0.277292 * 2637, metric = 11.38% * 2637;
 Minibatch[ 301- 400]: loss = 0.282939 * 2421, metric = 11.73% * 2421;
 Minibatch[ 401- 500]: loss = 0.298635 * 2406, metric = 12.97% * 2406;
 Minibatch[ 501- 600]: loss = 0.283756 * 2685, metric = 12.40% * 2685;
 Minibatch[ 601- 700]: loss = 0.292819 * 2566, metric = 11.77% * 2566;
 Minibatch[ 701- 800]: loss = 0.300500 * 2560, metric = 11.99% * 2560;
 Minibatch[ 801- 900]: loss = 0.276508 * 2652, metric = 11.31% * 2652;
 Minibatch[ 901-1000]: loss = 0.281309 * 2641, metric = 11.78% * 2641;
 Minibatch[1001-1100]: loss = 0.280549 * 2594, metric = 12.41% * 2594;
 Minibatch[1101-1200]: loss = 0.302456 * 2537, metric = 11.63% * 2537;
 Minibatch[1201-1300]: loss = 0.290618 * 2620, metric = 12.02% * 2620;
 Minibatch[1301-1400]: loss = 0.280052 * 2681, metric = 11.79% * 2681;
 Minibatch[1401-1500]: loss = 0.298090 * 2538, metric = 12.69% * 2538;
 Minibatch[1501-1600]: loss = 0.277098 * 2618, metric = 11.65% * 2618;
 Minibatch[1601-1700]: loss = 0.272286 * 2695, metric = 11.24% * 2695;
 Minibatch[1701-1800]: loss = 0.282547 * 2499, metric = 12.08% * 2499;
 Minibatch[1801-1900]: loss = 0.270660 * 2574, metric = 11.27% * 2574;
 Minibatch[1901-2000]: loss = 0.293238 * 2626, metric = 12.15% * 2626;
 Minibatch[2001-2100]: loss = 0.276180 * 2619, metric = 11.30% * 2619;
 Minibatch[2101-2200]: loss = 0.267402 * 2619, metric = 11.57% * 2619;
 Minibatch[2201-2300]: loss = 0.286006 * 2571, metric = 11.12% * 2571;
 Minibatch[2301-2400]: loss = 0.266579 * 2509, metric = 10.64% * 2509;
 Minibatch[2401-2500]: loss = 0.284256 * 2613, metric = 11.33% * 2613;
 Minibatch[2501-2600]: loss = 0.299725 * 2606, metric = 12.16% * 2606;
 Minibatch[2601-2700]: loss = 0.270651 * 2591, metric = 10.07% * 2591;
 Minibatch[2701-2800]: loss = 0.267097 * 2588, metric = 11.28% * 2588;
 Minibatch[2801-2900]: loss = 0.267882 * 2650, metric = 11.36% * 2650;
 Minibatch[2901-3000]: loss = 0.262855 * 2604, metric = 11.02% * 2604;
 Minibatch[3001-3100]: loss = 0.295492 * 2592, metric = 12.54% * 2592;
 Minibatch[3101-3200]: loss = 0.291074 * 2574, metric = 12.51% * 2574;
 Minibatch[3201-3300]: loss = 0.273974 * 2605, metric = 11.55% * 2605;
 Minibatch[3301-3400]: loss = 0.263924 * 2645, metric = 10.89% * 2645;
Finished Epoch[9 of 20]: [Training] loss = 0.283613 * 90000, metric = 11.78% * 90000 122.188s (736.6 samples/s);
 Evaluation Minibatch[   1- 500]: metric = 11.22% * 13062;
 Evaluation Minibatch[ 501-1000]: metric = 10.94% * 12994;
 Evaluation Minibatch[1001-1500]: metric = 11.52% * 12958;
 Evaluation Minibatch[1501-2000]: metric = 10.78% * 13178;
 Evaluation Minibatch[2001-2500]: metric = 11.65% * 12833;
Finished Evaluation [9]: Minibatch[1-2637]: metric = 11.23% * 68517;
 Minibatch[   1- 100]: loss = 0.256400 * 2659, metric = 10.94% * 2659;
 Minibatch[ 101- 200]: loss = 0.262070 * 2627, metric = 10.77% * 2627;
 Minibatch[ 201- 300]: loss = 0.295552 * 2703, metric = 11.84% * 2703;
 Minibatch[ 301- 400]: loss = 0.277965 * 2582, metric = 11.08% * 2582;
 Minibatch[ 401- 500]: loss = 0.289416 * 2463, metric = 12.79% * 2463;
 Minibatch[ 501- 600]: loss = 0.242403 * 2775, metric = 9.69% * 2775;
 Minibatch[ 601- 700]: loss = 0.272938 * 2582, metric = 11.46% * 2582;
 Minibatch[ 701- 800]: loss = 0.270957 * 2542, metric = 11.45% * 2542;
 Minibatch[ 801- 900]: loss = 0.255499 * 2622, metric = 10.49% * 2622;
 Minibatch[ 901-1000]: loss = 0.252334 * 2492, metric = 10.39% * 2492;
 Minibatch[1001-1100]: loss = 0.254628 * 2677, metric = 10.80% * 2677;
 Minibatch[1101-1200]: loss = 0.268751 * 2653, metric = 11.23% * 2653;
 Minibatch[1201-1300]: loss = 0.254734 * 2585, metric = 11.06% * 2585;
 Minibatch[1301-1400]: loss = 0.257393 * 2600, metric = 10.81% * 2600;
 Minibatch[1401-1500]: loss = 0.262125 * 2589, metric = 10.12% * 2589;
 Minibatch[1501-1600]: loss = 0.253527 * 2667, metric = 10.57% * 2667;
 Minibatch[1601-1700]: loss = 0.265102 * 2851, metric = 10.87% * 2851;
 Minibatch[1701-1800]: loss = 0.265937 * 2622, metric = 10.79% * 2622;
 Minibatch[1801-1900]: loss = 0.237331 * 2640, metric = 9.36% * 2640;
 Minibatch[1901-2000]: loss = 0.260523 * 2492, metric = 10.87% * 2492;
 Minibatch[2001-2100]: loss = 0.248942 * 2744, metric = 10.42% * 2744;
 Minibatch[2101-2200]: loss = 0.261992 * 2540, metric = 11.30% * 2540;
 Minibatch[2201-2300]: loss = 0.252335 * 2708, metric = 10.41% * 2708;
 Minibatch[2301-2400]: loss = 0.273687 * 2488, metric = 11.62% * 2488;
 Minibatch[2401-2500]: loss = 0.262471 * 2673, metric = 9.58% * 2673;
 Minibatch[2501-2600]: loss = 0.268387 * 2626, metric = 11.08% * 2626;
 Minibatch[2601-2700]: loss = 0.256224 * 2478, metric = 10.73% * 2478;
 Minibatch[2701-2800]: loss = 0.264987 * 2609, metric = 11.04% * 2609;
 Minibatch[2801-2900]: loss = 0.246906 * 2669, metric = 10.08% * 2669;
 Minibatch[2901-3000]: loss = 0.268975 * 2574, metric = 10.61% * 2574;
 Minibatch[3001-3100]: loss = 0.258147 * 2667, metric = 11.02% * 2667;
 Minibatch[3101-3200]: loss = 0.263152 * 2499, metric = 10.76% * 2499;
 Minibatch[3201-3300]: loss = 0.263745 * 2419, metric = 10.87% * 2419;
 Minibatch[3301-3400]: loss = 0.259427 * 2621, metric = 11.33% * 2621;
Finished Epoch[10 of 20]: [Training] loss = 0.262144 * 89995, metric = 10.85% * 89995 113.138s (795.4 samples/s);
 Evaluation Minibatch[   1- 500]: metric = 11.11% * 13062;
 Evaluation Minibatch[ 501-1000]: metric = 10.91% * 12994;
 Evaluation Minibatch[1001-1500]: metric = 11.29% * 12958;
 Evaluation Minibatch[1501-2000]: metric = 10.68% * 13178;
 Evaluation Minibatch[2001-2500]: metric = 11.17% * 12833;
Finished Evaluation [10]: Minibatch[1-2637]: metric = 11.05% * 68517;
 Minibatch[   1- 100]: loss = 0.270332 * 2528, metric = 12.03% * 2528;
 Minibatch[ 101- 200]: loss = 0.264604 * 2560, metric = 10.90% * 2560;
 Minibatch[ 201- 300]: loss = 0.265730 * 2592, metric = 11.03% * 2592;
 Minibatch[ 301- 400]: loss = 0.261025 * 2681, metric = 11.04% * 2681;
 Minibatch[ 401- 500]: loss = 0.248384 * 2512, metric = 9.83% * 2512;
 Minibatch[ 501- 600]: loss = 0.269986 * 2661, metric = 11.61% * 2661;
 Minibatch[ 601- 700]: loss = 0.270512 * 2508, metric = 10.96% * 2508;
 Minibatch[ 701- 800]: loss = 0.259235 * 2548, metric = 10.56% * 2548;
 Minibatch[ 801- 900]: loss = 0.262251 * 2605, metric = 10.44% * 2605;
 Minibatch[ 901-1000]: loss = 0.268650 * 2509, metric = 11.44% * 2509;
 Minibatch[1001-1100]: loss = 0.285024 * 2677, metric = 12.29% * 2677;
 Minibatch[1101-1200]: loss = 0.276614 * 2494, metric = 10.51% * 2494;
 Minibatch[1201-1300]: loss = 0.260572 * 2648, metric = 10.91% * 2648;
 Minibatch[1301-1400]: loss = 0.274009 * 2772, metric = 11.26% * 2772;
 Minibatch[1401-1500]: loss = 0.262375 * 2624, metric = 10.82% * 2624;
 Minibatch[1501-1600]: loss = 0.259837 * 2614, metric = 11.13% * 2614;
 Minibatch[1601-1700]: loss = 0.245840 * 2690, metric = 9.74% * 2690;
 Minibatch[1701-1800]: loss = 0.252207 * 2489, metric = 9.96% * 2489;
 Minibatch[1801-1900]: loss = 0.250975 * 2529, metric = 9.65% * 2529;
 Minibatch[1901-2000]: loss = 0.278312 * 2650, metric = 11.92% * 2650;
 Minibatch[2001-2100]: loss = 0.266863 * 2584, metric = 11.38% * 2584;
 Minibatch[2101-2200]: loss = 0.279814 * 2583, metric = 11.69% * 2583;
 Minibatch[2201-2300]: loss = 0.258833 * 2528, metric = 11.19% * 2528;
 Minibatch[2301-2400]: loss = 0.256168 * 2599, metric = 10.23% * 2599;
 Minibatch[2401-2500]: loss = 0.269804 * 2690, metric = 11.78% * 2690;
 Minibatch[2501-2600]: loss = 0.268673 * 2470, metric = 11.17% * 2470;
 Minibatch[2601-2700]: loss = 0.275797 * 2486, metric = 11.34% * 2486;
 Minibatch[2701-2800]: loss = 0.274662 * 2662, metric = 11.42% * 2662;
 Minibatch[2801-2900]: loss = 0.257970 * 2473, metric = 10.72% * 2473;
 Minibatch[2901-3000]: loss = 0.287794 * 2569, metric = 12.07% * 2569;
 Minibatch[3001-3100]: loss = 0.271697 * 2507, metric = 11.41% * 2507;
 Minibatch[3101-3200]: loss = 0.267450 * 2492, metric = 11.04% * 2492;
 Minibatch[3201-3300]: loss = 0.262565 * 2492, metric = 10.96% * 2492;
 Minibatch[3301-3400]: loss = 0.254913 * 2580, metric = 10.74% * 2580;
Finished Epoch[11 of 20]: [Training] loss = 0.265953 * 90000, metric = 11.04% * 90000 121.488s (740.8 samples/s);
 Evaluation Minibatch[   1- 500]: metric = 11.40% * 13062;
 Evaluation Minibatch[ 501-1000]: metric = 11.05% * 12994;
 Evaluation Minibatch[1001-1500]: metric = 11.00% * 12958;
 Evaluation Minibatch[1501-2000]: metric = 11.17% * 13178;
 Evaluation Minibatch[2001-2500]: metric = 11.21% * 12833;
Finished Evaluation [11]: Minibatch[1-2637]: metric = 11.17% * 68517;
 Minibatch[   1- 100]: loss = 0.264226 * 2529, metric = 10.64% * 2529;
 Minibatch[ 101- 200]: loss = 0.246656 * 2444, metric = 9.98% * 2444;
 Minibatch[ 201- 300]: loss = 0.262156 * 2619, metric = 10.54% * 2619;
 Minibatch[ 301- 400]: loss = 0.251505 * 2720, metric = 10.62% * 2720;
 Minibatch[ 401- 500]: loss = 0.253521 * 2741, metric = 10.40% * 2741;
 Minibatch[ 501- 600]: loss = 0.261735 * 2594, metric = 10.56% * 2594;
 Minibatch[ 601- 700]: loss = 0.265431 * 2579, metric = 10.90% * 2579;
 Minibatch[ 701- 800]: loss = 0.292122 * 2596, metric = 12.25% * 2596;
 Minibatch[ 801- 900]: loss = 0.288875 * 2556, metric = 11.31% * 2556;
 Minibatch[ 901-1000]: loss = 0.261054 * 2504, metric = 10.62% * 2504;
 Minibatch[1001-1100]: loss = 0.281879 * 2538, metric = 11.66% * 2538;
 Minibatch[1101-1200]: loss = 0.271286 * 2600, metric = 11.54% * 2600;
 Minibatch[1201-1300]: loss = 0.284751 * 2664, metric = 11.64% * 2664;
 Minibatch[1301-1400]: loss = 0.266032 * 2531, metric = 10.94% * 2531;
 Minibatch[1401-1500]: loss = 0.269886 * 2579, metric = 10.90% * 2579;
 Minibatch[1501-1600]: loss = 0.271812 * 2723, metric = 11.16% * 2723;
 Minibatch[1601-1700]: loss = 0.273175 * 2526, metric = 11.60% * 2526;
 Minibatch[1701-1800]: loss = 0.278621 * 2694, metric = 11.54% * 2694;
 Minibatch[1801-1900]: loss = 0.263559 * 2640, metric = 10.68% * 2640;
 Minibatch[1901-2000]: loss = 0.268221 * 2566, metric = 10.25% * 2566;
 Minibatch[2001-2100]: loss = 0.269739 * 2558, metric = 11.14% * 2558;
 Minibatch[2101-2200]: loss = 0.261861 * 2668, metric = 10.72% * 2668;
 Minibatch[2201-2300]: loss = 0.278066 * 2654, metric = 11.61% * 2654;
 Minibatch[2301-2400]: loss = 0.274260 * 2518, metric = 11.87% * 2518;
 Minibatch[2401-2500]: loss = 0.260704 * 2588, metric = 10.90% * 2588;
 Minibatch[2501-2600]: loss = 0.258541 * 2618, metric = 10.35% * 2618;
 Minibatch[2601-2700]: loss = 0.250791 * 2548, metric = 10.32% * 2548;
 Minibatch[2701-2800]: loss = 0.249374 * 2624, metric = 10.14% * 2624;
 Minibatch[2801-2900]: loss = 0.267400 * 2582, metric = 11.39% * 2582;
 Minibatch[2901-3000]: loss = 0.248570 * 2714, metric = 9.87% * 2714;
 Minibatch[3001-3100]: loss = 0.257367 * 2734, metric = 10.17% * 2734;
 Minibatch[3101-3200]: loss = 0.257063 * 2660, metric = 11.32% * 2660;
 Minibatch[3201-3300]: loss = 0.260069 * 2522, metric = 10.90% * 2522;
 Minibatch[3301-3400]: loss = 0.261841 * 2420, metric = 11.32% * 2420;
Finished Epoch[12 of 20]: [Training] loss = 0.265810 * 90016, metric = 10.93% * 90016 111.829s (804.9 samples/s);
 Evaluation Minibatch[   1- 500]: metric = 10.39% * 13062;
 Evaluation Minibatch[ 501-1000]: metric = 10.36% * 12994;
 Evaluation Minibatch[1001-1500]: metric = 10.44% * 12958;
 Evaluation Minibatch[1501-2000]: metric = 10.15% * 13178;
 Evaluation Minibatch[2001-2500]: metric = 10.73% * 12833;
Finished Evaluation [12]: Minibatch[1-2637]: metric = 10.41% * 68517;
 Minibatch[   1- 100]: loss = 0.261775 * 2530, metric = 10.51% * 2530;
 Minibatch[ 101- 200]: loss = 0.262022 * 2481, metric = 11.08% * 2481;
 Minibatch[ 201- 300]: loss = 0.265896 * 2693, metric = 11.10% * 2693;
 Minibatch[ 301- 400]: loss = 0.248579 * 2513, metric = 9.71% * 2513;
 Minibatch[ 401- 500]: loss = 0.268169 * 2538, metric = 10.99% * 2538;
 Minibatch[ 501- 600]: loss = 0.267657 * 2543, metric = 11.09% * 2543;
 Minibatch[ 601- 700]: loss = 0.261785 * 2637, metric = 10.66% * 2637;
 Minibatch[ 701- 800]: loss = 0.247843 * 2513, metric = 9.91% * 2513;
 Minibatch[ 801- 900]: loss = 0.245173 * 2570, metric = 9.22% * 2570;
 Minibatch[ 901-1000]: loss = 0.254809 * 2721, metric = 10.14% * 2721;
 Minibatch[1001-1100]: loss = 0.240920 * 2634, metric = 9.61% * 2634;
 Minibatch[1101-1200]: loss = 0.254393 * 2463, metric = 11.00% * 2463;
 Minibatch[1201-1300]: loss = 0.235667 * 2484, metric = 9.94% * 2484;
 Minibatch[1301-1400]: loss = 0.245137 * 2461, metric = 8.90% * 2461;
 Minibatch[1401-1500]: loss = 0.236244 * 2535, metric = 9.39% * 2535;
 Minibatch[1501-1600]: loss = 0.246807 * 2616, metric = 10.02% * 2616;
 Minibatch[1601-1700]: loss = 0.230579 * 2701, metric = 9.14% * 2701;
 Minibatch[1701-1800]: loss = 0.237043 * 2629, metric = 9.05% * 2629;
 Minibatch[1801-1900]: loss = 0.239595 * 2491, metric = 9.59% * 2491;
 Minibatch[1901-2000]: loss = 0.249453 * 2544, metric = 9.47% * 2544;
 Minibatch[2001-2100]: loss = 0.251219 * 2462, metric = 10.07% * 2462;
 Minibatch[2101-2200]: loss = 0.241156 * 2515, metric = 9.58% * 2515;
 Minibatch[2201-2300]: loss = 0.277635 * 2575, metric = 11.03% * 2575;
 Minibatch[2301-2400]: loss = 0.227832 * 2676, metric = 9.19% * 2676;
 Minibatch[2401-2500]: loss = 0.261340 * 2450, metric = 10.86% * 2450;
 Minibatch[2501-2600]: loss = 0.255457 * 2536, metric = 10.61% * 2536;
 Minibatch[2601-2700]: loss = 0.234162 * 2633, metric = 9.42% * 2633;
 Minibatch[2701-2800]: loss = 0.259966 * 2508, metric = 10.96% * 2508;
 Minibatch[2801-2900]: loss = 0.240648 * 2481, metric = 9.67% * 2481;
 Minibatch[2901-3000]: loss = 0.247016 * 2643, metric = 9.99% * 2643;
 Minibatch[3001-3100]: loss = 0.254054 * 2507, metric = 10.33% * 2507;
 Minibatch[3101-3200]: loss = 0.250220 * 2559, metric = 10.59% * 2559;
 Minibatch[3201-3300]: loss = 0.232479 * 2682, metric = 9.28% * 2682;
 Minibatch[3301-3400]: loss = 0.236328 * 2664, metric = 9.65% * 2664;
 Minibatch[3401-3500]: loss = 0.244733 * 2722, metric = 10.21% * 2722;
Finished Epoch[13 of 20]: [Training] loss = 0.248820 * 89984, metric = 10.05% * 89984 121.955s (737.8 samples/s);
 Evaluation Minibatch[   1- 500]: metric = 10.90% * 13062;
 Evaluation Minibatch[ 501-1000]: metric = 10.52% * 12994;
 Evaluation Minibatch[1001-1500]: metric = 10.51% * 12958;
 Evaluation Minibatch[1501-2000]: metric = 10.67% * 13178;
 Evaluation Minibatch[2001-2500]: metric = 10.88% * 12833;
Finished Evaluation [13]: Minibatch[1-2637]: metric = 10.70% * 68517;
 Minibatch[   1- 100]: loss = 0.245805 * 2599, metric = 10.16% * 2599;
 Minibatch[ 101- 200]: loss = 0.226746 * 2557, metric = 9.07% * 2557;
 Minibatch[ 201- 300]: loss = 0.255765 * 2450, metric = 9.96% * 2450;
 Minibatch[ 301- 400]: loss = 0.254894 * 2740, metric = 9.93% * 2740;
 Minibatch[ 401- 500]: loss = 0.232610 * 2637, metric = 9.44% * 2637;
 Minibatch[ 501- 600]: loss = 0.258363 * 2650, metric = 10.72% * 2650;
 Minibatch[ 601- 700]: loss = 0.226495 * 2674, metric = 9.31% * 2674;
 Minibatch[ 701- 800]: loss = 0.241202 * 2592, metric = 8.95% * 2592;
 Minibatch[ 801- 900]: loss = 0.280528 * 2504, metric = 12.22% * 2504;
 Minibatch[ 901-1000]: loss = 0.261909 * 2628, metric = 10.88% * 2628;
 Minibatch[1001-1100]: loss = 0.239900 * 2425, metric = 9.65% * 2425;
 Minibatch[1101-1200]: loss = 0.254283 * 2547, metric = 10.40% * 2547;
 Minibatch[1201-1300]: loss = 0.252964 * 2574, metric = 10.30% * 2574;
 Minibatch[1301-1400]: loss = 0.261168 * 2645, metric = 10.55% * 2645;
 Minibatch[1401-1500]: loss = 0.261088 * 2429, metric = 10.29% * 2429;
 Minibatch[1501-1600]: loss = 0.240037 * 2552, metric = 9.56% * 2552;
 Minibatch[1601-1700]: loss = 0.246751 * 2588, metric = 9.66% * 2588;
 Minibatch[1701-1800]: loss = 0.256028 * 2671, metric = 10.26% * 2671;
 Minibatch[1801-1900]: loss = 0.256093 * 2752, metric = 10.47% * 2752;
 Minibatch[1901-2000]: loss = 0.256729 * 2554, metric = 9.63% * 2554;
 Minibatch[2001-2100]: loss = 0.271286 * 2499, metric = 11.04% * 2499;
 Minibatch[2101-2200]: loss = 0.271026 * 2539, metric = 10.79% * 2539;
 Minibatch[2201-2300]: loss = 0.260613 * 2598, metric = 10.35% * 2598;
 Minibatch[2301-2400]: loss = 0.242225 * 2585, metric = 10.41% * 2585;
 Minibatch[2401-2500]: loss = 0.237243 * 2641, metric = 9.66% * 2641;
 Minibatch[2501-2600]: loss = 0.242623 * 2641, metric = 10.11% * 2641;
 Minibatch[2601-2700]: loss = 0.253236 * 2580, metric = 9.77% * 2580;
 Minibatch[2701-2800]: loss = 0.263997 * 2553, metric = 10.81% * 2553;
 Minibatch[2801-2900]: loss = 0.255052 * 2552, metric = 10.42% * 2552;
 Minibatch[2901-3000]: loss = 0.251631 * 2483, metric = 11.12% * 2483;
 Minibatch[3001-3100]: loss = 0.266335 * 2626, metric = 10.97% * 2626;
 Minibatch[3101-3200]: loss = 0.240733 * 2626, metric = 10.05% * 2626;
 Minibatch[3201-3300]: loss = 0.235895 * 2400, metric = 9.88% * 2400;
 Minibatch[3301-3400]: loss = 0.247798 * 2531, metric = 10.90% * 2531;
Finished Epoch[14 of 20]: [Training] loss = 0.251075 * 90007, metric = 10.21% * 90007 120.640s (746.1 samples/s);
 Evaluation Minibatch[   1- 500]: metric = 10.40% * 13062;
 Evaluation Minibatch[ 501-1000]: metric = 10.91% * 12994;
 Evaluation Minibatch[1001-1500]: metric = 10.66% * 12958;
 Evaluation Minibatch[1501-2000]: metric = 10.27% * 13178;
 Evaluation Minibatch[2001-2500]: metric = 10.62% * 12833;
Finished Evaluation [14]: Minibatch[1-2637]: metric = 10.60% * 68517;
 Minibatch[   1- 100]: loss = 0.253188 * 2672, metric = 10.33% * 2672;
 Minibatch[ 101- 200]: loss = 0.241396 * 2555, metric = 10.02% * 2555;
 Minibatch[ 201- 300]: loss = 0.246018 * 2519, metric = 10.00% * 2519;
 Minibatch[ 301- 400]: loss = 0.256669 * 2769, metric = 10.73% * 2769;
 Minibatch[ 401- 500]: loss = 0.241078 * 2530, metric = 9.96% * 2530;
 Minibatch[ 501- 600]: loss = 0.236725 * 2612, metric = 9.30% * 2612;
 Minibatch[ 601- 700]: loss = 0.255202 * 2713, metric = 10.03% * 2713;
 Minibatch[ 701- 800]: loss = 0.239120 * 2620, metric = 10.00% * 2620;
 Minibatch[ 801- 900]: loss = 0.238584 * 2698, metric = 9.71% * 2698;
 Minibatch[ 901-1000]: loss = 0.256587 * 2574, metric = 10.61% * 2574;
 Minibatch[1001-1100]: loss = 0.233175 * 2713, metric = 9.10% * 2713;
 Minibatch[1101-1200]: loss = 0.260694 * 2583, metric = 10.61% * 2583;
 Minibatch[1201-1300]: loss = 0.243661 * 2536, metric = 10.02% * 2536;
 Minibatch[1301-1400]: loss = 0.242734 * 2601, metric = 9.96% * 2601;
 Minibatch[1401-1500]: loss = 0.251696 * 2470, metric = 10.73% * 2470;
 Minibatch[1501-1600]: loss = 0.258598 * 2605, metric = 10.25% * 2605;
 Minibatch[1601-1700]: loss = 0.243094 * 2608, metric = 9.89% * 2608;
 Minibatch[1701-1800]: loss = 0.252875 * 2655, metric = 10.55% * 2655;
 Minibatch[1801-1900]: loss = 0.226226 * 2533, metric = 9.36% * 2533;
 Minibatch[1901-2000]: loss = 0.262468 * 2540, metric = 10.67% * 2540;
 Minibatch[2001-2100]: loss = 0.230178 * 2552, metric = 9.44% * 2552;
 Minibatch[2101-2200]: loss = 0.241564 * 2728, metric = 10.34% * 2728;
 Minibatch[2201-2300]: loss = 0.245953 * 2631, metric = 10.07% * 2631;
 Minibatch[2301-2400]: loss = 0.237823 * 2557, metric = 9.66% * 2557;
 Minibatch[2401-2500]: loss = 0.238993 * 2637, metric = 10.16% * 2637;
 Minibatch[2501-2600]: loss = 0.232198 * 2567, metric = 8.77% * 2567;
 Minibatch[2601-2700]: loss = 0.257568 * 2734, metric = 10.94% * 2734;
 Minibatch[2701-2800]: loss = 0.243100 * 2685, metric = 10.24% * 2685;
 Minibatch[2801-2900]: loss = 0.244509 * 2647, metric = 9.71% * 2647;
 Minibatch[2901-3000]: loss = 0.243024 * 2752, metric = 8.98% * 2752;
 Minibatch[3001-3100]: loss = 0.254601 * 2666, metric = 10.80% * 2666;
 Minibatch[3101-3200]: loss = 0.230402 * 2446, metric = 9.89% * 2446;
 Minibatch[3201-3300]: loss = 0.257025 * 2671, metric = 10.45% * 2671;
 Minibatch[3301-3400]: loss = 0.254018 * 2624, metric = 10.79% * 2624;
Finished Epoch[15 of 20]: [Training] loss = 0.245540 * 90004, metric = 10.06% * 90004 111.365s (808.2 samples/s);
 Evaluation Minibatch[   1- 500]: metric = 10.29% * 13062;
 Evaluation Minibatch[ 501-1000]: metric = 10.07% * 12994;
 Evaluation Minibatch[1001-1500]: metric = 10.08% * 12958;
 Evaluation Minibatch[1501-2000]: metric = 9.99% * 13178;
 Evaluation Minibatch[2001-2500]: metric = 10.02% * 12833;
Finished Evaluation [15]: Minibatch[1-2637]: metric = 10.11% * 68517;
 Minibatch[   1- 100]: loss = 0.241570 * 2577, metric = 9.43% * 2577;
 Minibatch[ 101- 200]: loss = 0.224200 * 2611, metric = 9.50% * 2611;
 Minibatch[ 201- 300]: loss = 0.222820 * 2644, metric = 9.04% * 2644;
 Minibatch[ 301- 400]: loss = 0.227690 * 2744, metric = 9.48% * 2744;
 Minibatch[ 401- 500]: loss = 0.261882 * 2662, metric = 10.44% * 2662;
 Minibatch[ 501- 600]: loss = 0.247940 * 2532, metric = 10.47% * 2532;
 Minibatch[ 601- 700]: loss = 0.237058 * 2695, metric = 9.35% * 2695;
 Minibatch[ 701- 800]: loss = 0.236741 * 2592, metric = 9.30% * 2592;
 Minibatch[ 801- 900]: loss = 0.246701 * 2622, metric = 10.60% * 2622;
 Minibatch[ 901-1000]: loss = 0.233305 * 2643, metric = 9.42% * 2643;
 Minibatch[1001-1100]: loss = 0.228341 * 2662, metric = 8.90% * 2662;
 Minibatch[1101-1200]: loss = 0.248206 * 2624, metric = 9.83% * 2624;
 Minibatch[1201-1300]: loss = 0.230709 * 2522, metric = 9.00% * 2522;
 Minibatch[1301-1400]: loss = 0.247172 * 2522, metric = 10.47% * 2522;
 Minibatch[1401-1500]: loss = 0.220081 * 2485, metric = 8.77% * 2485;
 Minibatch[1501-1600]: loss = 0.220885 * 2650, metric = 8.91% * 2650;
 Minibatch[1601-1700]: loss = 0.217778 * 2618, metric = 8.29% * 2618;
 Minibatch[1701-1800]: loss = 0.229372 * 2618, metric = 8.98% * 2618;
 Minibatch[1801-1900]: loss = 0.228781 * 2692, metric = 9.06% * 2692;
 Minibatch[1901-2000]: loss = 0.227693 * 2676, metric = 8.59% * 2676;
 Minibatch[2001-2100]: loss = 0.211166 * 2549, metric = 8.00% * 2549;
 Minibatch[2101-2200]: loss = 0.229413 * 2633, metric = 9.08% * 2633;
 Minibatch[2201-2300]: loss = 0.239217 * 2464, metric = 9.25% * 2464;
 Minibatch[2301-2400]: loss = 0.209705 * 2503, metric = 8.79% * 2503;
 Minibatch[2401-2500]: loss = 0.233344 * 2693, metric = 8.95% * 2693;
 Minibatch[2501-2600]: loss = 0.206582 * 2443, metric = 8.39% * 2443;
 Minibatch[2601-2700]: loss = 0.216616 * 2700, metric = 8.52% * 2700;
 Minibatch[2701-2800]: loss = 0.232235 * 2541, metric = 9.52% * 2541;
 Minibatch[2801-2900]: loss = 0.231329 * 2623, metric = 9.11% * 2623;
 Minibatch[2901-3000]: loss = 0.244915 * 2665, metric = 9.53% * 2665;
 Minibatch[3001-3100]: loss = 0.242815 * 2669, metric = 9.89% * 2669;
 Minibatch[3101-3200]: loss = 0.247792 * 2396, metric = 10.39% * 2396;
 Minibatch[3201-3300]: loss = 0.235954 * 2673, metric = 9.39% * 2673;
 Minibatch[3301-3400]: loss = 0.209080 * 2725, metric = 8.18% * 2725;
Finished Epoch[16 of 20]: [Training] loss = 0.231692 * 89994, metric = 9.27% * 89994 113.586s (792.3 samples/s);
 Evaluation Minibatch[   1- 500]: metric = 10.20% * 13062;
 Evaluation Minibatch[ 501-1000]: metric = 10.34% * 12994;
 Evaluation Minibatch[1001-1500]: metric = 10.37% * 12958;
 Evaluation Minibatch[1501-2000]: metric = 10.14% * 13178;
 Evaluation Minibatch[2001-2500]: metric = 10.13% * 12833;
Finished Evaluation [16]: Minibatch[1-2637]: metric = 10.24% * 68517;
 Minibatch[   1- 100]: loss = 0.243892 * 2536, metric = 9.42% * 2536;
 Minibatch[ 101- 200]: loss = 0.234181 * 2504, metric = 9.07% * 2504;
 Minibatch[ 201- 300]: loss = 0.242222 * 2486, metric = 10.06% * 2486;
 Minibatch[ 301- 400]: loss = 0.236148 * 2536, metric = 9.35% * 2536;
 Minibatch[ 401- 500]: loss = 0.227647 * 2589, metric = 9.46% * 2589;
 Minibatch[ 501- 600]: loss = 0.223724 * 2585, metric = 9.09% * 2585;
 Minibatch[ 601- 700]: loss = 0.231851 * 2523, metric = 9.67% * 2523;
 Minibatch[ 701- 800]: loss = 0.230621 * 2625, metric = 9.26% * 2625;
 Minibatch[ 801- 900]: loss = 0.241524 * 2590, metric = 10.15% * 2590;
 Minibatch[ 901-1000]: loss = 0.225409 * 2702, metric = 8.59% * 2702;
 Minibatch[1001-1100]: loss = 0.247709 * 2437, metric = 11.08% * 2437;
 Minibatch[1101-1200]: loss = 0.230869 * 2792, metric = 9.20% * 2792;
 Minibatch[1201-1300]: loss = 0.254036 * 2444, metric = 10.11% * 2444;
 Minibatch[1301-1400]: loss = 0.249822 * 2518, metric = 9.69% * 2518;
 Minibatch[1401-1500]: loss = 0.238563 * 2627, metric = 9.48% * 2627;
 Minibatch[1501-1600]: loss = 0.252428 * 2640, metric = 10.42% * 2640;
 Minibatch[1601-1700]: loss = 0.247521 * 2583, metric = 10.34% * 2583;
 Minibatch[1701-1800]: loss = 0.235847 * 2513, metric = 9.27% * 2513;
 Minibatch[1801-1900]: loss = 0.218518 * 2603, metric = 8.84% * 2603;
 Minibatch[1901-2000]: loss = 0.241022 * 2741, metric = 9.16% * 2741;
 Minibatch[2001-2100]: loss = 0.236719 * 2628, metric = 9.59% * 2628;
 Minibatch[2101-2200]: loss = 0.225184 * 2685, metric = 8.90% * 2685;
 Minibatch[2201-2300]: loss = 0.238801 * 2487, metric = 9.61% * 2487;
 Minibatch[2301-2400]: loss = 0.226971 * 2605, metric = 9.94% * 2605;
 Minibatch[2401-2500]: loss = 0.211491 * 2581, metric = 8.37% * 2581;
 Minibatch[2501-2600]: loss = 0.245069 * 2560, metric = 9.92% * 2560;
 Minibatch[2601-2700]: loss = 0.240855 * 2502, metric = 9.11% * 2502;
 Minibatch[2701-2800]: loss = 0.212567 * 2616, metric = 8.98% * 2616;
 Minibatch[2801-2900]: loss = 0.218872 * 2670, metric = 8.73% * 2670;
 Minibatch[2901-3000]: loss = 0.232929 * 2614, metric = 9.91% * 2614;
 Minibatch[3001-3100]: loss = 0.245979 * 2629, metric = 9.47% * 2629;
 Minibatch[3101-3200]: loss = 0.245155 * 2647, metric = 9.52% * 2647;
 Minibatch[3201-3300]: loss = 0.222724 * 2837, metric = 9.31% * 2837;
 Minibatch[3301-3400]: loss = 0.222205 * 2712, metric = 9.22% * 2712;
Finished Epoch[17 of 20]: [Training] loss = 0.234397 * 89989, metric = 9.47% * 89989 119.896s (750.6 samples/s);
 Evaluation Minibatch[   1- 500]: metric = 10.34% * 13062;
 Evaluation Minibatch[ 501-1000]: metric = 10.70% * 12994;
 Evaluation Minibatch[1001-1500]: metric = 10.63% * 12958;
 Evaluation Minibatch[1501-2000]: metric = 10.36% * 13178;
 Evaluation Minibatch[2001-2500]: metric = 10.08% * 12833;
Finished Evaluation [17]: Minibatch[1-2637]: metric = 10.44% * 68517;
 Minibatch[   1- 100]: loss = 0.237914 * 2697, metric = 9.79% * 2697;
 Minibatch[ 101- 200]: loss = 0.229046 * 2590, metric = 9.19% * 2590;
 Minibatch[ 201- 300]: loss = 0.207391 * 2654, metric = 8.21% * 2654;
 Minibatch[ 301- 400]: loss = 0.254026 * 2520, metric = 9.76% * 2520;
 Minibatch[ 401- 500]: loss = 0.252770 * 2711, metric = 10.25% * 2711;
 Minibatch[ 501- 600]: loss = 0.243798 * 2424, metric = 10.68% * 2424;
 Minibatch[ 601- 700]: loss = 0.234505 * 2527, metric = 9.14% * 2527;
 Minibatch[ 701- 800]: loss = 0.237155 * 2515, metric = 9.50% * 2515;
 Minibatch[ 801- 900]: loss = 0.224854 * 2619, metric = 8.74% * 2619;
 Minibatch[ 901-1000]: loss = 0.234626 * 2610, metric = 9.50% * 2610;
 Minibatch[1001-1100]: loss = 0.239985 * 2419, metric = 9.63% * 2419;
 Minibatch[1101-1200]: loss = 0.254770 * 2564, metric = 9.95% * 2564;
 Minibatch[1201-1300]: loss = 0.232133 * 2719, metric = 9.56% * 2719;
 Minibatch[1301-1400]: loss = 0.232685 * 2582, metric = 9.53% * 2582;
 Minibatch[1401-1500]: loss = 0.244764 * 2572, metric = 9.95% * 2572;
 Minibatch[1501-1600]: loss = 0.222210 * 2490, metric = 9.16% * 2490;
 Minibatch[1601-1700]: loss = 0.241629 * 2582, metric = 10.03% * 2582;
 Minibatch[1701-1800]: loss = 0.238278 * 2432, metric = 9.91% * 2432;
 Minibatch[1801-1900]: loss = 0.228964 * 2570, metric = 9.34% * 2570;
 Minibatch[1901-2000]: loss = 0.228651 * 2571, metric = 9.10% * 2571;
 Minibatch[2001-2100]: loss = 0.224234 * 2684, metric = 9.02% * 2684;
 Minibatch[2101-2200]: loss = 0.255768 * 2549, metric = 10.20% * 2549;
 Minibatch[2201-2300]: loss = 0.245912 * 2651, metric = 10.18% * 2651;
 Minibatch[2301-2400]: loss = 0.243178 * 2559, metric = 9.53% * 2559;
 Minibatch[2401-2500]: loss = 0.254929 * 2703, metric = 9.88% * 2703;
 Minibatch[2501-2600]: loss = 0.213965 * 2533, metric = 9.08% * 2533;
 Minibatch[2601-2700]: loss = 0.245753 * 2493, metric = 9.63% * 2493;
 Minibatch[2701-2800]: loss = 0.237174 * 2711, metric = 9.33% * 2711;
 Minibatch[2801-2900]: loss = 0.240356 * 2706, metric = 9.76% * 2706;
 Minibatch[2901-3000]: loss = 0.226101 * 2566, metric = 8.92% * 2566;
 Minibatch[3001-3100]: loss = 0.226331 * 2601, metric = 8.88% * 2601;
 Minibatch[3101-3200]: loss = 0.241399 * 2630, metric = 9.54% * 2630;
 Minibatch[3201-3300]: loss = 0.228051 * 2562, metric = 8.86% * 2562;
 Minibatch[3301-3400]: loss = 0.263011 * 2434, metric = 9.98% * 2434;
Finished Epoch[18 of 20]: [Training] loss = 0.237642 * 90014, metric = 9.55% * 90014 115.503s (779.3 samples/s);
 Evaluation Minibatch[   1- 500]: metric = 10.47% * 13062;
 Evaluation Minibatch[ 501-1000]: metric = 10.30% * 12994;
 Evaluation Minibatch[1001-1500]: metric = 10.51% * 12958;
 Evaluation Minibatch[1501-2000]: metric = 10.28% * 13178;
 Evaluation Minibatch[2001-2500]: metric = 9.66% * 12833;
Finished Evaluation [18]: Minibatch[1-2637]: metric = 10.27% * 68517;
 Minibatch[   1- 100]: loss = 0.259618 * 2593, metric = 10.49% * 2593;
 Minibatch[ 101- 200]: loss = 0.250185 * 2607, metric = 10.74% * 2607;
 Minibatch[ 201- 300]: loss = 0.237197 * 2657, metric = 9.86% * 2657;
 Minibatch[ 301- 400]: loss = 0.232075 * 2456, metric = 9.36% * 2456;
 Minibatch[ 401- 500]: loss = 0.272271 * 2484, metric = 10.10% * 2484;
 Minibatch[ 501- 600]: loss = 0.227584 * 2720, metric = 9.34% * 2720;
 Minibatch[ 601- 700]: loss = 0.230386 * 2631, metric = 9.62% * 2631;
 Minibatch[ 701- 800]: loss = 0.239813 * 2597, metric = 9.86% * 2597;
 Minibatch[ 801- 900]: loss = 0.237343 * 2603, metric = 9.68% * 2603;
 Minibatch[ 901-1000]: loss = 0.253969 * 2654, metric = 10.25% * 2654;
 Minibatch[1001-1100]: loss = 0.234987 * 2571, metric = 9.41% * 2571;
 Minibatch[1101-1200]: loss = 0.204550 * 2528, metric = 7.95% * 2528;
 Minibatch[1201-1300]: loss = 0.228344 * 2625, metric = 9.52% * 2625;
 Minibatch[1301-1400]: loss = 0.209502 * 2664, metric = 8.41% * 2664;
 Minibatch[1401-1500]: loss = 0.215041 * 2741, metric = 8.76% * 2741;
 Minibatch[1501-1600]: loss = 0.221860 * 2673, metric = 9.28% * 2673;
 Minibatch[1601-1700]: loss = 0.229497 * 2667, metric = 9.34% * 2667;
 Minibatch[1701-1800]: loss = 0.231909 * 2587, metric = 9.16% * 2587;
 Minibatch[1801-1900]: loss = 0.233668 * 2328, metric = 9.06% * 2328;
 Minibatch[1901-2000]: loss = 0.228872 * 2662, metric = 8.83% * 2662;
 Minibatch[2001-2100]: loss = 0.219885 * 2668, metric = 9.00% * 2668;
 Minibatch[2101-2200]: loss = 0.245179 * 2376, metric = 9.93% * 2376;
 Minibatch[2201-2300]: loss = 0.223638 * 2542, metric = 9.21% * 2542;
 Minibatch[2301-2400]: loss = 0.227494 * 2616, metric = 8.79% * 2616;
 Minibatch[2401-2500]: loss = 0.227250 * 2642, metric = 9.20% * 2642;
 Minibatch[2501-2600]: loss = 0.251961 * 2465, metric = 9.45% * 2465;
 Minibatch[2601-2700]: loss = 0.210833 * 2502, metric = 7.95% * 2502;
 Minibatch[2701-2800]: loss = 0.227859 * 2640, metric = 9.17% * 2640;
 Minibatch[2801-2900]: loss = 0.231352 * 2669, metric = 9.67% * 2669;
 Minibatch[2901-3000]: loss = 0.215798 * 2635, metric = 8.43% * 2635;
 Minibatch[3001-3100]: loss = 0.197215 * 2702, metric = 7.62% * 2702;
 Minibatch[3101-3200]: loss = 0.231557 * 2230, metric = 9.24% * 2230;
 Minibatch[3201-3300]: loss = 0.239392 * 2600, metric = 9.08% * 2600;
 Minibatch[3301-3400]: loss = 0.219032 * 2635, metric = 8.69% * 2635;
Finished Epoch[19 of 20]: [Training] loss = 0.230853 * 90002, metric = 9.26% * 90002 118.214s (761.3 samples/s);
 Evaluation Minibatch[   1- 500]: metric = 13.32% * 13062;
 Evaluation Minibatch[ 501-1000]: metric = 12.93% * 12994;
 Evaluation Minibatch[1001-1500]: metric = 13.13% * 12958;
 Evaluation Minibatch[1501-2000]: metric = 13.33% * 13178;
 Evaluation Minibatch[2001-2500]: metric = 12.94% * 12833;
Finished Evaluation [19]: Minibatch[1-2637]: metric = 13.12% * 68517;
 Minibatch[   1- 100]: loss = 0.238204 * 2547, metric = 9.38% * 2547;
 Minibatch[ 101- 200]: loss = 0.208840 * 2560, metric = 7.89% * 2560;
 Minibatch[ 201- 300]: loss = 0.240625 * 2588, metric = 9.78% * 2588;
 Minibatch[ 301- 400]: loss = 0.248512 * 2639, metric = 9.36% * 2639;
 Minibatch[ 401- 500]: loss = 0.224534 * 2672, metric = 8.42% * 2672;
 Minibatch[ 501- 600]: loss = 0.219207 * 2572, metric = 8.63% * 2572;
 Minibatch[ 601- 700]: loss = 0.214189 * 2584, metric = 8.59% * 2584;
 Minibatch[ 701- 800]: loss = 0.235974 * 2524, metric = 9.03% * 2524;
 Minibatch[ 801- 900]: loss = 0.249309 * 2483, metric = 10.27% * 2483;
 Minibatch[ 901-1000]: loss = 0.253803 * 2643, metric = 9.91% * 2643;
 Minibatch[1001-1100]: loss = 0.233040 * 2583, metric = 9.76% * 2583;
 Minibatch[1101-1200]: loss = 0.255334 * 2593, metric = 10.07% * 2593;
 Minibatch[1201-1300]: loss = 0.218658 * 2596, metric = 8.28% * 2596;
 Minibatch[1301-1400]: loss = 0.227413 * 2563, metric = 9.01% * 2563;
 Minibatch[1401-1500]: loss = 0.237677 * 2533, metric = 8.92% * 2533;
 Minibatch[1501-1600]: loss = 0.239061 * 2519, metric = 9.49% * 2519;
 Minibatch[1601-1700]: loss = 0.216575 * 2604, metric = 8.41% * 2604;
 Minibatch[1701-1800]: loss = 0.227859 * 2605, metric = 9.25% * 2605;
 Minibatch[1801-1900]: loss = 0.230897 * 2640, metric = 9.28% * 2640;
 Minibatch[1901-2000]: loss = 0.242163 * 2649, metric = 9.74% * 2649;
 Minibatch[2001-2100]: loss = 0.244224 * 2600, metric = 9.15% * 2600;
 Minibatch[2101-2200]: loss = 0.230677 * 2717, metric = 9.16% * 2717;
 Minibatch[2201-2300]: loss = 0.246769 * 2559, metric = 9.03% * 2559;
 Minibatch[2301-2400]: loss = 0.228166 * 2687, metric = 9.12% * 2687;
 Minibatch[2401-2500]: loss = 0.231814 * 2608, metric = 9.70% * 2608;
 Minibatch[2501-2600]: loss = 0.234391 * 2656, metric = 9.68% * 2656;
 Minibatch[2601-2700]: loss = 0.239851 * 2532, metric = 9.32% * 2532;
 Minibatch[2701-2800]: loss = 0.221869 * 2694, metric = 9.02% * 2694;
 Minibatch[2801-2900]: loss = 0.230770 * 2673, metric = 9.43% * 2673;
 Minibatch[2901-3000]: loss = 0.244973 * 2621, metric = 9.84% * 2621;
 Minibatch[3001-3100]: loss = 0.250073 * 2690, metric = 9.96% * 2690;
 Minibatch[3101-3200]: loss = 0.243114 * 2658, metric = 9.48% * 2658;
 Minibatch[3201-3300]: loss = 0.244933 * 2599, metric = 10.00% * 2599;
 Minibatch[3301-3400]: loss = 0.237047 * 2536, metric = 8.99% * 2536;
 Minibatch[3401-3500]: loss = 0.244013 * 1433, metric = 9.49% * 1433;
Finished Epoch[20 of 20]: [Training] loss = 0.235240 * 89978, metric = 9.28% * 89978 115.169s (781.3 samples/s);
 Evaluation Minibatch[   1- 500]: metric = 13.00% * 13062;
 Evaluation Minibatch[ 501-1000]: metric = 13.31% * 12994;
 Evaluation Minibatch[1001-1500]: metric = 13.18% * 12958;
 Evaluation Minibatch[1501-2000]: metric = 13.17% * 13178;
 Evaluation Minibatch[2001-2500]: metric = 12.64% * 12833;
Finished Evaluation [20]: Minibatch[1-2637]: metric = 13.09% * 68517;
